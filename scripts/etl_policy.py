# scripts/etl_policy_v2.py
import asyncio
import os
import sys
import glob  # å¼•å…¥ glob ç”¨äºæ‰¹é‡æŸ¥æ‰¾æ–‡ä»¶

sys.path.append(os.getcwd())

# å¼•å…¥ PDF åŠ è½½å™¨
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from sqlalchemy import delete
from tenacity import retry, stop_after_attempt, wait_exponential # å¼•å…¥é‡è¯•åº“
from pydantic import SecretStr
from app.core.config import settings
from app.core.database import async_session_maker
from app.models.knowledge import KnowledgeChunk

# ================= é…ç½®åŒº =================
BATCH_SIZE = 50  # æ¯æ¬¡å‘ Embedding API å‘é€çš„ç‰‡æ®µæ•°é‡ï¼ˆé˜²æ­¢ API è¶…æ—¶ï¼‰
DB_BATCH_SIZE = 100 # æ¯æ¬¡å‘æ•°æ®åº“å†™å…¥çš„æ¡æ•°
# =========================================

embeddings_model = OpenAIEmbeddings(
    base_url=settings.OPENAI_BASE_URL,
    api_key=SecretStr(settings.OPENAI_API_KEY),
    model=settings.EMBEDDING_MODEL,
    check_embedding_ctx_length=False
)

def get_loader(file_path: str):
    """æ ¹æ®æ–‡ä»¶åç¼€è‡ªåŠ¨é€‰æ‹©åŠ è½½å™¨"""
    ext = os.path.splitext(file_path)[1].lower()
    if ext == ".pdf":
        return PyPDFLoader(file_path)
    elif ext in [".md", ".txt"]:
        return TextLoader(file_path, encoding='utf-8')
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {ext}")

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
async def embed_with_retry(texts):
    """å¸¦é‡è¯•æœºåˆ¶çš„ Embedding è°ƒç”¨ï¼Œé˜²æ­¢ç½‘ç»œæŠ–åŠ¨"""
    return await embeddings_model.aembed_documents(texts)

async def process_file(file_path: str, source_name: str):
    print(f"ğŸš€ [Start] å¤„ç†æ–‡ä»¶: {source_name}")
    
    try:
        # --- Step 1: åŠ è½½ ---
        loader = get_loader(file_path)
        # load() æ˜¯åŒæ­¥çš„ï¼Œå¦‚æœæ–‡ä»¶å·¨å¤§å»ºè®®ç”¨ lazy_load()ï¼Œè¿™é‡Œç®€å•èµ·è§ç”¨ load
        docs = loader.load()
        
        # --- Step 2: åˆ‡ç‰‡ ---
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""]
        )
        split_docs = text_splitter.split_documents(docs)
        total_chunks = len(split_docs)
        print(f"  ğŸ“„ åˆ‡åˆ†å®Œæˆ: {total_chunks} ä¸ªç‰‡æ®µ")

        if total_chunks == 0:
            print("  âš ï¸ è­¦å‘Š: æ–‡ä»¶ä¸ºç©ºæˆ–æ— æ³•è¯»å–")
            return

        async with async_session_maker() as session:
            # --- Step 3.1: å¹‚ç­‰æ¸…ç† ---
            # åªæœ‰åœ¨ç¬¬ä¸€æ‰¹æ¬¡æ—¶æ‰æ¸…ç†ï¼Œæˆ–è€…ä¸€æ¬¡æ€§æ¸…ç†
            print(f"  broom æ¸…ç†æ—§æ•°æ®...")
            await session.execute(delete(KnowledgeChunk).where(KnowledgeChunk.source == source_name))
            await session.commit() # ç«‹å³æäº¤åˆ é™¤

            # --- Step 3.2: åˆ†æ‰¹å¤„ç† (Batch Processing) ---
            # è¿™æ˜¯è§„é¿ OOM å’Œ API è¶…æ—¶çš„å…³é”®ï¼
            for i in range(0, total_chunks, BATCH_SIZE):
                batch_docs = split_docs[i : i + BATCH_SIZE]
                
                # ğŸ› ï¸ [ä¿®å¤æ ¸å¿ƒ]ï¼šæå–æ–‡æœ¬çš„åŒæ—¶ï¼Œè¿‡æ»¤æ‰ç©ºå­—ç¬¦ä¸²å’Œçº¯ç©ºç™½å­—ç¬¦
                batch_texts = []
                valid_indices = [] # è®°å½•æœ‰æ•ˆæ–‡æœ¬å¯¹åº”çš„åŸå§‹ç´¢å¼•ï¼Œä»¥ä¾¿å¯¹é½ Metadata
                
                for idx, doc in enumerate(batch_docs):
                    cleaned_text = doc.page_content.strip() # å»é™¤é¦–å°¾ç©ºæ ¼
                    if cleaned_text:  # åªæœ‰éç©ºæ–‡æœ¬æ‰å¤„ç†
                        batch_texts.append(cleaned_text) # æ³¨æ„ï¼šè¿™é‡Œç”¨æ¸…æ´—åçš„æ–‡æœ¬è¿˜æ˜¯åŸæ–‡æœ¬è§†éœ€æ±‚è€Œå®šï¼Œé€šå¸¸æ¸…æ´—åçš„æ›´å¥½
                        valid_indices.append(idx)

                # å¦‚æœè¿™ä¸€æ‰¹å…¨æ˜¯ç©ºè¡Œï¼Œç›´æ¥è·³è¿‡ï¼Œä¸ç„¶ API ä¼šæŠ¥é”™
                if not batch_texts:
                    print(f"  âš ï¸ è·³è¿‡ç©ºç™½æ‰¹æ¬¡ {i}")
                    continue

                # æå–å¯¹åº”çš„ MetaData (åªå–æœ‰æ•ˆçš„)
                batch_metas = []
                for idx in valid_indices:
                    doc = batch_docs[idx]
                    page = doc.metadata.get("page", 0) + 1 
                    # chunk_index ä¾ç„¶åŸºäºå…¨å±€çš„ i + idx
                    batch_metas.append({"page": page, "chunk_index": i + idx})

                print(f"  ğŸ§  Embedding æ‰¹æ¬¡ {i // BATCH_SIZE + 1} (æœ‰æ•ˆç‰‡æ®µ: {len(batch_texts)})...")
                
                # è°ƒç”¨å¸¦é‡è¯•çš„ Embedding
                vectors = await embed_with_retry(batch_texts)

                # ç»„è£…å¯¹è±¡
                new_chunks = []
                for j, text in enumerate(batch_texts):
                    chunk = KnowledgeChunk(
                        content=text,
                        embedding=vectors[j],
                        source=source_name,
                        meta_data=batch_metas[j]
                    )
                    new_chunks.append(chunk)
                
                # å†™å…¥æ•°æ®åº“
                session.add_all(new_chunks)
                await session.commit() # åˆ†æ‰¹æäº¤ï¼Œé‡Šæ”¾æ•°æ®åº“å‹åŠ›
                
        print(f"âœ… [Done] {source_name} å¤„ç†å®Œæ¯•")

    except Exception as e:
        print(f"âŒ [Error] å¤„ç†æ–‡ä»¶ {file_path} å¤±è´¥: {str(e)}")
        # ç”Ÿäº§ç¯å¢ƒä¸­è¿™é‡Œåº”è¯¥è®°å½•åˆ° error.logï¼Œè€Œä¸æ˜¯ä»…ä»… print

async def main():
    # 1. è‡ªåŠ¨æ‰«æ data ç›®å½•ä¸‹çš„æ‰€æœ‰ PDF å’Œ MD æ–‡ä»¶
    # å‡è®¾ä½ çš„æ–‡ä»¶éƒ½åœ¨é¡¹ç›®æ ¹ç›®å½•çš„ data æ–‡ä»¶å¤¹ä¸‹
    base_dir = "data"
    
    # æŸ¥æ‰¾ pdf, md, txt
    all_files = []
    for ext in ["*.pdf", "*.md", "*.txt"]:
        all_files.extend(glob.glob(os.path.join(base_dir, ext)))

    print(f"ğŸ“‚ æ‰«æåˆ° {len(all_files)} ä¸ªæ–‡ä»¶å¾…å¤„ç†...")

    for file_path in all_files:
        # è‡ªåŠ¨ç”Ÿæˆ source_nameï¼Œä¾‹å¦‚ "data/policy.pdf" -> "policy.pdf"
        source_name = os.path.basename(file_path)
        
        await process_file(file_path, source_name)

if __name__ == "__main__":
    # Windows ä¸‹å¦‚æœæŠ¥é”™ EventLoop ç›¸å…³é—®é¢˜ï¼Œå¯ä»¥è§£å¼€ä¸‹é¢è¿™è¡Œçš„æ³¨é‡Š
    # asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())